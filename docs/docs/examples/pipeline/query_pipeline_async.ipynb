{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd032bcb-fefb-48ec-94da-08d49ac26120",
   "metadata": {},
   "source": [
    "# Query Pipeline with Async/Parallel Execution\n",
    "\n",
    "Here we showcase our query pipeline with async + parallel execution.\n",
    "\n",
    "We do this by setting up a RAG pipeline that does the following:\n",
    "1. Send query to multiple RAG query engines.\n",
    "2. Combine results.\n",
    "\n",
    "In the process we'll also show some nice abstractions for joining results (e.g. our `ArgPackComponent()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531eedc-4f65-457e-8844-55fcc1773154",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load in the Paul Graham essay as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a4f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a441905-9007-44d6-b71a-6fc3e5023e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-10 12:31:00--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75042 (73K) [text/plain]\n",
      "Saving to: ‘pg_essay.txt’\n",
      "\n",
      "pg_essay.txt        100%[===================>]  73.28K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2024-01-10 12:31:00 (6.32 MB/s) - ‘pg_essay.txt’ saved [75042/75042]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt' -O pg_essay.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533149c-4312-4444-9b45-52afe21731ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=[\"pg_essay.txt\"])\n",
    "documents = reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d5ff8-ae04-4ea3-bbe0-2c097af71efd",
   "metadata": {},
   "source": [
    "## Setup Query Pipeline\n",
    "\n",
    "We setup a parallel query pipeline that executes multiple chunk sizes at once, and combines the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63caf998-0a88-4c50-b6a4-2a0c412bde5b",
   "metadata": {},
   "source": [
    "### Define Modules\n",
    "\n",
    "This includes:\n",
    "- LLM\n",
    "- Chunk Sizes\n",
    "- Query Engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fcbdb2-6747-4e65-b1ce-5d40febccb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import (\n",
    "    QueryPipeline,\n",
    "    InputComponent,\n",
    "    ArgPackComponent,\n",
    ")\n",
    "from typing import Dict, Any, List, Optional\n",
    "from llama_index.core.llama_pack import BaseLlamaPack\n",
    "from llama_index.core.llms import LLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core.schema import NodeWithScore, TextNode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "chunk_sizes = [128, 256, 512, 1024]\n",
    "query_engines = {}\n",
    "for chunk_size in chunk_sizes:\n",
    "    splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    query_engines[str(chunk_size)] = vector_index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87a439-88e6-4130-b28f-45268330d3e4",
   "metadata": {},
   "source": [
    "### Construct Query Pipeline\n",
    "\n",
    "Connect input to multiple query engines, and join the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff95be2e-517f-4632-a7b8-a2e0dec11d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct query pipeline\n",
    "p = QueryPipeline(verbose=True)\n",
    "module_dict = {\n",
    "    **query_engines,\n",
    "    \"input\": InputComponent(),\n",
    "    \"summarizer\": TreeSummarize(),\n",
    "    \"join\": ArgPackComponent(\n",
    "        convert_fn=lambda x: NodeWithScore(node=TextNode(text=str(x)))\n",
    "    ),\n",
    "}\n",
    "p.add_modules(module_dict)\n",
    "# add links from input to query engine (id'ed by chunk_size)\n",
    "for chunk_size in chunk_sizes:\n",
    "    p.add_link(\"input\", str(chunk_size))\n",
    "    p.add_link(str(chunk_size), \"join\", dest_key=str(chunk_size))\n",
    "p.add_link(\"join\", \"summarizer\", dest_key=\"nodes\")\n",
    "p.add_link(\"input\", \"summarizer\", dest_key=\"query_str\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda05274-09c5-4b56-b2ba-57f445346e73",
   "metadata": {},
   "source": [
    "## Try out Queries\n",
    "\n",
    "Let's compare the async performance vs. synchronous performance!\n",
    "\n",
    "In our experiments we get a 2x speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e161ce-ef10-446f-acfb-f6d3a1d291bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running modules and inputs in parallel: \n",
      "Module key: input. Input: \n",
      "input: What did the author do during his time in YC?\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running modules and inputs in parallel: \n",
      "Module key: 128. Input: \n",
      "input: What did the author do during his time in YC?\n",
      "\n",
      "Module key: 256. Input: \n",
      "input: What did the author do during his time in YC?\n",
      "\n",
      "Module key: 512. Input: \n",
      "input: What did the author do during his time in YC?\n",
      "\n",
      "Module key: 1024. Input: \n",
      "input: What did the author do during his time in YC?\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running modules and inputs in parallel: \n",
      "Module key: join. Input: \n",
      "128: The author worked on solving the problems of startups that were part of the YC program.\n",
      "256: The author worked on YC's internal software in Arc and also wrote essays during his time in YC.\n",
      "512: During his time in YC, the author worked on various projects. Initially, he intended to do three things: hack, write essays, and work on YC. However, as YC grew and he became more excited about it, it...\n",
      "1024: During his time in YC, the author worked on YC's internal software in Arc and wrote essays. He also worked on various projects related to YC, such as helping startups and solving their problems. Addit...\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running modules and inputs in parallel: \n",
      "Module key: summarizer. Input: \n",
      "query_str: What did the author do during his time in YC?\n",
      "nodes: [NodeWithScore(node=TextNode(id_='7e0b0aeb-04e3-4518-b534-2cf68c07ae1f', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='fe9144af45...\n",
      "\n",
      "\n",
      "\u001b[0mDuring his time in YC, the author worked on various projects, including YC's internal software in Arc and writing essays. He also helped startups and solved their problems, and was involved in disputes between cofounders. Additionally, the author worked hard to ensure the success of YC and dealt with people who maltreated startups.\n",
      "Time taken: 3.943013906478882\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "response = await p.arun(input=\"What did the author do during his time in YC?\")\n",
    "print(str(response))\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b36ba65-636f-4fe9-8dee-e318cfe9a50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "input: What did the author do during his time in YC?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 128 with input: \n",
      "input: What did the author do during his time in YC?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 256 with input: \n",
      "input: What did the author do during his time in YC?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 512 with input: \n",
      "input: What did the author do during his time in YC?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module 1024 with input: \n",
      "input: What did the author do during his time in YC?\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module join with input: \n",
      "128: The author worked on solving the problems of startups that were part of the YC program.\n",
      "256: The author worked on YC's internal software in Arc and also wrote essays.\n",
      "512: During his time in YC, the author worked on various projects. Initially, he intended to do three things: hack, write essays, and work on YC. However, as YC grew and he became more excited about it, it...\n",
      "1024: During his time in YC, the author worked on YC's internal software in Arc, wrote essays, and worked on various projects related to YC. He also engaged in solving the problems faced by startups that we...\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module summarizer with input: \n",
      "query_str: What did the author do during his time in YC?\n",
      "nodes: [NodeWithScore(node=TextNode(id_='4d698e2f-811e-42ce-bd0d-9b5615b0bbfd', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='fe9144af45...\n",
      "\n",
      "\u001b[0mDuring his time in YC, the author worked on YC's internal software in Arc, wrote essays, and worked on various projects related to YC. He also engaged in solving the problems faced by startups that were part of YC's program. Additionally, the author mentioned working on tasks he didn't particularly enjoy, such as resolving disputes between cofounders and dealing with people who mistreated startups.\n",
      "Time taken: 7.640604019165039\n"
     ]
    }
   ],
   "source": [
    "# compare with sync method\n",
    "\n",
    "start_time = time.time()\n",
    "response = p.run(input=\"What did the author do during his time in YC?\")\n",
    "print(str(response))\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_v2",
   "language": "python",
   "name": "llama_index_v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
